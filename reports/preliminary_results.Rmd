---
title: "Preliminary results"
author: "Erik Bulow"
date: '2019-02-25'
output: 
  html_document:
    df_print: "kable"
    toc: true
    toc_float: true
    code_folding: "hide"
abstract: |
  This is a background description of some analysis made within the project. 
  It is written in English due to planned collaboration with NJR.
  I have started with prevouis analysis made by Szilard but have used new data and slightly different methods.
  I have only considered death within 90 days so far.
bibliography: P:/library.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
load("../cache/df.RData")
library(tidyverse)
options(digits = 2)
```

# Data

I have used data from the "new linkage data base". Primary prosthesis inserted 2008-2015 were included. Note that we should have no problem with censoring here since death dates are recorded up until February 2018. Hence even if patients were operated during 2015, they are not automatically censored at the end of that year. (Censoring would only occour in the case of emigration etc.)
Comorbidity indices based on the sum of comorbidities recognized by Elixhauser et al. (ECI), Charlson's original index (CCI) and a modified Risk Rx IV index (modified by Anne Garland) were used. Each of those indices were truncated since higher values are rare. 

The following inclusion/exclusion criteria were applied:

1. Only primaries (no re-operations).
2. Only OA.
3. No hemi.
4. No second bilaterals.
5. No first bilateral if second bilateral inserted within 90 days.
6. No surface prosthesis
7. Only age 18-100 years
8. only BMI $\leq 50$
9. Only ASA $\leq 5$
10. Exclude if missing education level
11. Exclude if missing marital status level
12. Exclude if missing type of hospital
13. Exclude if fixation data missing

No imputation applied.

Exact numbers of patients/hips from each inclusion/exclusion step will be calculated after everything is set. 
The final data set has `r nrow(df)` rows with `r ncol (df)` columns:

```{r}
names(df)
```


education = 

* low = pre-gymnasium
* middle = gymnasium
* high = post-gymnasium

civil_status = married/single/widow(er). Registered partnership etc are treated as marriage.

We have also used some individual comorbidities recognized by ECI, CCI and Rx and combined those to identify individual comorbidities using an arbitrary source. Those variables are included with suffixes "c_" to their variable names. 


## Data profiling report

A data profiling report is find as an appendix.
Some comments:

### Basic statistics

Most of our variables are categorical/binary.

### Missing data 

We do not have any missing data (due to our selection criteria)


### Univariate distributions (numeric variables)

BMI and age are approximately normally distributed. There are some negative ECI values as well (due to the van Walraven index).


### Bar Chart (categorical variables)

Type of hospital, sex, ASA, civil status, education and looks as expected.

We see that there are very few patients with each comorbidity (hypertension is an exception, as well as some more commonly used medications). This is problematic since logistic regression works best with outcome proportions of at least 20 %.

No one gets medically treated for dementia.

### QQ PLot

Those plots indicate (again) that age and BMI are approximately normally distributed, while some other variables are not.

We can see as well that individuals who dies within 90 days have higher ECI, CCI and age on average.


### Corelation analysis

We have too many variables to distinguish. Obviously, each variable is strongly correlated to itself (diagonal line). A cluster of correlated variables are also found in the upper right corner since outcome variables appear in different forms in the data set. We can also assume that some comorbidities are associated and obviously so for the same comorbidity measured by different indices. 

### Bivariate distribution

Older people are over-represented among the dead.


## Table 1


Note that some EPVs are low, thus that we have very few observed detahs among some levels! This can make modelleing hard. There is no perfect limit of cases, but 10-20 is commonly recommended and  [@Courvoisier2011] suggested to have at least 5!

```{r}
library(tableone)
load("../cache/t1.RData")
 t1 %>%
  print(
    showAllLevels = TRUE,
    printToggle = FALSE
  ) %>%
  as_tibble(rownames = "what")
```

## Table 2

We can also make a table similar to table 2. So far we only include univariable models.
Most of the result is consistent with the previous version of the table.

```{r}
load("../cache/t2_univariable.RData")
t2_univariable
```


# Modelling

## Risk of overfitting

We have a lot of potential predictors but very few events to predict (low EPV). This is problematic and we risk overfittnig the model to the data. Hence, if we develop/train our model using all data and then use the same data to evaluate predictive power, the result will look better than it actually is (applying the model to new data will not work as well).

We can use some proportion of our data as a hold-out sample to use for later validation. It is common to use 75 % of the data for model training and 25 % for evaluation. We can choose however to use 90 % for training and 10 % for evaluation. With the rare event rate, a totally random 90/10-split might lead to extremely few "positive outcomes" (deaths) in the evaluation set. The sampling is therefore stratified on the outcome. 

To have such a small estimation set implies however that the observed outcome will be very rare, and the result of validating the model on that data set will be unstable. In this case, the split-sample approach might in fact lead to more confusion. It has been argued by [@Austin2017a] that Bootsrtap-validation is superiod in this case. Another prominent predictive modelling guru, Max Kuhn however, does argue for the need of a separate validation test. The approach used for this project should be further discussed!


## Downsamling

The training set will now have approximately `r round(nrow(df) * .9)` patients. Only about 0.26 % of those will die within 90 days. A secure bet then will be to assume that everyone will survive. Such a model will have perfect specificity. The sensitivity will of course be 0, but most likely it will anyway if the difference between patients who dies and survive is not clear-cut. Hence, if we are to use any sort of step-wise regression modelling, the estimated coefficients for a model based on all this data, will underestimate the true coefficient values (the model will be too conservative) [@King2001].

A common and simple solution to this problem is to use down-sampling. This is done by matching if the goal is to compare treatments or similar. We can not use matching in our case (we are not comparing groups based on variables that we have not yet selected). We can however simply include all cases and then as many controls (survivors). 

An alternative to down-sampling is up-sampling or a combination of the two [@Lunardon2014].
An additional benefit of down-sampling however is that we will also reduce the sample size (without losing too much power). This is beneficial since step-wise regression techniques with around 100 variables and a large data set will otherwise be very time consuming.

With a balanced data set, we can no longer cheat by predicting everyone as survivors without a severe loss of specificity. We therefore force the model to do a much better job (not underestimating the beta coefficients in the logistic regression model).


# Uncertainty 

A draw-back with the down-sampling however is that we might get a sample that is not representative for the larger population. Our estimates might be very sensitive to the individuals who just happened to end up in the sample. If we perform a step-wise regression on two different samples, it would therefore be possible that each data set will provide two different sets of models (based on different predictors) [@Lukacs2010; @Sauerbrei1999; @Zellner2004]. 

A theoretically simple (although computationally intense) method is to repeat this procedure several times with different samples. We will do that.


## Candidate predictors
 
I took 1,000 bootstrap replicates of the training data and applied down-sampling to each of those. 
For each sample I used the Aikake Information Criteria as a selection criteria within step-wise regression. I saved a list of the selected variables for each "winning model". Note that the size of the coefficients for those variables are not yet important. The same variable can even have a positive effect in one model, but a negative effect in another (depending on which other variables are included, thus which part of the population would constitute the base line).

Note that variables that are highly correlated could not be included in the same selection step, since it will then be arbitrary which one of those to select. I therefore used four independent step-wise regression procedures to each re-sample:

1. General variables (including our newly combined comorbidities)
2. Individual ECI diagnosis
3. Individual CCI diagnosis
4. Individual Risk Rx IV diagnosis


The following table list all assessed variables ordered by the proportion of times for which they were included in the final model chosen by the step-wise procedure. 


```{r}
load("../cache/prop_selected.RData")
DT::datatable(prop_selected)
```

It is common to use univariable screening before the step-wise procedure, which was here considered redundant since univariable models are already included in the step-wise procedure (see [@Harrell2015]).

I also tried the Bayesian Information Criteria (BIC) instead of AIC, but it promotes simpler models quite heavily in this case, wherefore fewer predictors will end up in the final model. I did not find this to be desirable yet.

We can see from the table above that most potential predictors end up in less than half of all models. We can illustrate the distribution of proportions for a more intuitive overview:

```{r}
ggplot(prop_selected, aes(value)) + 
  geom_density(fill = "lightblue") + 
  theme_minimal()
```

It is usually recommended to only include variables that are in fact chosen in around 90 %
 (or even 95 % of the cases). We could use a 75 % limit however since this is not our final model.
 


We now have the following candidate predictors:

```{r}
load("../cache/candidates.RData")
candidates
```

It was actually only ASA = 3 that showed up above (level 2 could not be distinguished from level 1) but we include both levels any way (the data is already available so why not use it?).


# Combined model 

The candidate variables above where found from four different starting points. It might therefore be relevant to use step-wise regression once more based on the smaller subset of candidate variables to account for possible dependencies.

It would be possible to do this on the whole training data set. We would still have the same uncertainty problem as above however. Instead we can use "bagging" (bootstrap aggregating), hence repeated bootstrap samples (again), with down-sampling performed to each re-sample and a full main effects model estimated for each re-sample. This is similar to above but the result from each sample is not only a set of candidate predictors but also estimates of each coefficient associated with those variables. Also, models of all possible combinations of potential predictors are tried out, not only the few where AIC were not increased. We get the following results:

```{r}
load("../cache/modsums.RData")
round(modsums$modsum[[1]]$coefmat.full, 2) %>% 
  as_tibble(rownames = "coef") %>% 
  mutate(Est_exp = round(exp(Estimate), 2))
```



# Additional models

We will compare our model to models with ASA, CCI, ECI and Risk Rx, as in the previous version of the manuscript. We can include a model with only age and sex as well.


# Predictive power

We now 6 models that can be compared based on predictive power, thus by area under the (ROC) curve (AUC).
Let's take another 1,000 bootstrap replicates and compare observations to predictions for those. Note that we do not only use the list of predictors for each model, but also the estimated coefficients. We thus evaluate the models from above, rather than fitting new ones and evaluate again. We do this to decrease overfitting. This will give us 1,000 ROC curves for each model. We could plot all of those but the resulting figure would be quite messy (and time consuming to draw). I therefore fitted a locally estimated scatter-plot smoothing (LOESS) line as an average ROC-curve based on individual bootstrap estimates.  

Overfitting will nevertheless occur since we are in fact still using the training data. We thus need to re-calculate AUC-values one final time for the evaluation data set, the 10 % of the original data that we have not looked at yet. We could apply Bootstrap replicates to the test data as well in order to estimate uncertainty/confidence intervals. Those would probably we very wide though and are avoided to save (computational) time. We could do this as a later step before summarising the results however.



# Results and interpretation

Averaged ROC-curves for each model is illustrated below. Curves below the dotted line would not occur if the models had been estimated and evaluated using the same data set. This is not the case here and we do in fact have models that perform worse then random. This happens for models with only a few possible values of its predictors, since those models are insensitive to nuances not captured by those limited values. This is also a clear sign of over-fitting (the model is too heavily biased to the initial data and the result can not be generalized to a larger population).

The funny looking shape of the ASA model is due to the fact that we have only three distinct value for this predictor. The model must therefore assign all patients with the same score as either dead or alive after 90 days. This is a blunt tool!

It is evident that ROC curves from the training data over-estimate the predictive power of each model, as well as that the differnece between models get over-estimated.



```{r}
knitr::include_graphics("../graphs/rocs.png")
```


We could  summaries the ROC curves by their area under the curve (AUC). We have used bootstrap replicates and can therefore select the 2.5 % smallest and largest estimates as confidence limits based on the training data. We can present the same data either in a table, or graphically.

```{r}
load("../cache/auc_table.RData")
auc_table
```



```{r}
knitr::include_graphics("../graphs/auc_ci.png")
```


We can conclude that the winning model have the same AUC as the chosen model from the previous version of the manuscript (0.8). We could also conclude however that this is an optimistic estimate since we do not reach that far using the test data. 

It might seem strange that both CCI and ECI actually underestimate the AUC. We can see above however that corresponding curves based on the evaluation data are very close to the diagonal with just a few exceptions. We might suspect that the larger training set included some extreme values for those indices that became influential for the model building, but which were not present in the evaluation set. We said in the beginning that a common split between training data and evaluation data is 3:1. We had 9:1 (which we knew were risky). We did so because the outcome is so rare and we therefore wanted to maximize the available data for estimation.

Lets have a look at the extremes of the Elixhauser index (CCI is less extreme but have a similar tendency). Most patientd have ECI within the range -7 to 21. Those are excluded here: 

```{r}
load("../cache/ECI_extremes.RData")
ECI_extremes %>% 
  filter(!ECI_index_sum_all %in% -7:21)
```

Hence, there is only one individual with ECI = -11, and she was assigned to the training set. She had obesity, drug abuse and depression, all associated with a long and happy life according to Van Walraven (Sic!). 

The patient with ECI = 33 on the other hand had congestive heart failure, cardiac arrhythmia, hypertension, renal failure and a solid cancer tumor with metastases.

Both of those patients actually survived 90 days after their surgery (which is an indication to why ECI does not work as a predictor of death)!

One way to avaoid extreme observations for ECI/CCI would be to transform those variables by the Yeo-Johnson transformation. I tried this but it did not help. It might therefore be reasonable to simply truncate the scale (or even ignore/exclude those extreme cases).



## Separation 

Another attempt to illustrate each models ability to classify patients as either dead or alive is depicted below. If a model was able to fully predict death, the blue and red areas would be completely separated. If the model can not predict anything at all, those colors would completely blend into each other. More blue to the right means that people who actually died where also predicted to do so. We can see once more that the separation is larger when the models are applied to the training data, compared to the evaluation data set.

This visualisation was just made up but a similar approach has been suggested by @Greenhill2011.

```{r}
knitr::include_graphics("../graphs/separation_auc.png")
```

# References
